import tensorflow as tf
import matplotlib.pyplot as plt
import pandas as pd
import numpy as np
from tqdm import tqdm
import warnings
from tensorflow.keras import layers
from tensorflow.keras.models import Model
warnings.filterwarnings('ignore')
tf.test.is_gpu_available()

(X_train,Y_train_or), (X_test, Y_test_or) = tf.keras.datasets.cifar10.load_data()

labels_name = ['飞机','汽车','鸟类','猫','鹿','狗','蛙类','马','船','卡车']

"""
压缩标签维度
"""
Y_train = np.squeeze(Y_train_or)
Y_test = np.squeeze(Y_test_or)

"""
对标签进行one_hat编码
"""
Y_train = np.eye(10)[Y_train]
Y_test = np.eye(10)[Y_test]

Y_train = Y_train.astype(np.float32)
Y_test = Y_test.astype(np.float32)

"""
对图片进行归一化操作
"""
X_train = X_train/255.0
X_test = X_test/255.0

"""
转换图像像素值的类型
"""
X_train = X_train.astype(np.float32)
X_test = X_test.astype(np.float32)

images_count = X_train.shape[0]
BATCH_SIZE = 64

ts_X_train = tf.data.Dataset.from_tensor_slices(X_train)
ts_Y_train = tf.data.Dataset.from_tensor_slices(Y_train)
ts_train_set = tf.data.Dataset.zip((ts_X_train,ts_Y_train))
train_dataset = ts_train_set.shuffle(images_count).batch(BATCH_SIZE)

def encoder():
    img_input = layers.Input(shape = (32,32,3))
    noise_input = layers.Input(shape = (32,32,3))
    
    encoder_input = layers.Concatenate()([img_input,noise_input])
    
    y = layers.Conv2D(32,[3,3],strides = 2,padding = "same",use_bias = False)(encoder_input)
    y = layers.Dropout(rate = 0.3)(y)
    y = layers.BatchNormalization()(y)
    y = layers.Activation("tanh")(y)
    
    y = layers.Conv2D(64,[3,3],strides = 2,padding = "same",use_bias = False)(y)
    y = layers.Dropout(rate = 0.3)(y)
    y = layers.BatchNormalization()(y)
    y = layers.Activation("tanh")(y)
    
    #AE的输出4*4*32
    y = layers.Conv2D(32,[3,3],strides = 2,padding = "same",use_bias = False)(y)
    fea_out = layers.BatchNormalization()(y)
    
    encoder_model = Model(inputs = [img_input,noise_input],outputs = fea_out)
    encoder_model.summary()
    return encoder_model
Encoder = encoder()

def decoder():  
    decoder_input = layers.Input(shape = (4,4,32))
    
    #4*4*128
    y = layers.Conv2DTranspose(128,[3,3],strides = 1,padding = "same",use_bias = False,kernel_initializer='random_normal')(decoder_input)
    y = layers.Dropout(rate = 0.3)(y)
    y = layers.BatchNormalization()(y)
    y = layers.Activation("tanh")(y)
    
    #8*8*128
    y = layers.Conv2DTranspose(128,[3,3],strides = 2,padding = "same",use_bias = False,kernel_initializer='random_normal')(y)
    y = layers.Dropout(rate = 0.3)(y)
    y = layers.BatchNormalization()(y)
    y = layers.Activation("tanh")(y)
    
    #16*16*128
    y = layers.Conv2DTranspose(128,[3,3],strides = 2,padding = "same",use_bias = False,kernel_initializer='random_normal')(y)
    y = layers.Dropout(rate = 0.3)(y)
    y = layers.BatchNormalization()(y)
    y = layers.Activation("tanh")(y)
    
    #32*32*3---对抗样本
    adv_img = layers.Conv2DTranspose(3,[5,5],strides = 2,padding = "same",use_bias = False,kernel_initializer='random_normal')(y)
    adv_img = layers.BatchNormalization()(adv_img)
    
    #32*32*3----对抗扰动
    adv_per = layers.Conv2DTranspose(3,[5,5],strides = 2,padding = "same",use_bias = False,kernel_initializer='random_normal')(y)
    adv_per = layers.BatchNormalization()(adv_per)
    #[-1,1]
    adv_per = layers.Activation("tanh")(adv_per)
    
    decoder_model = Model(inputs = decoder_input,outputs = [adv_img,adv_per])
    decoder_model.summary()
    return decoder_model
Decoder = decoder() 

"""
搭建判别器网络
"""
def discriminator():
    dis_input = layers.Input(shape = (32,32,3))
    
    x = layers.Conv2D(filters=32, kernel_size=3, strides=(2, 2))(dis_input)
    x = layers.Activation("relu")(x)
    
    x = layers.Conv2D(filters=64, kernel_size=3, strides=(2, 2))(x)
    x = layers.Activation("relu")(x)
    
    x = tf.keras.layers.Flatten()(x)    
    
    #判断真实样本，还是假样本
    x = layers.Dense(1)(x)
    
    dis_model = Model(inputs = dis_input,outputs = x)
    dis_model.summary()
    return dis_model
Discriminator = discriminator()

target_net_20 = tf.keras.models.load_model("D:/model_save/resnet20.h5")
target_net_32 = tf.keras.models.load_model("D:/model_save/resnet32.h5")
target_net_w28 = tf.keras.models.load_model("D:/model_save/wide_resnet28.h5")

def cul_attck_net(nums,target_net):
    sel_imgs = X_train[:nums]
    sel_labels = Y_train[:nums]
    input_noise = np.random.normal(size = sel_imgs.shape)
    en_output = Encoder.predict([sel_imgs,input_noise])
    #重构出对抗样本--对抗扰动
    imgs_fake,imgs_per = Decoder.predict(en_output)
    perturbation = tf.clip_by_value(imgs_per,-0.15,0.15)
    imgs_fake2 = tf.sigmoid(imgs_fake)
    last_img = tf.add(perturbation,imgs_fake2)
    adv_imgs = tf.clip_by_value(last_img,0.0,1.0)
    return 1 - target_net.evaluate(adv_imgs,sel_labels,verbose = 0)[1]

fool_loss_fun = tf.losses.CategoricalCrossentropy(from_logits=False)
dis_loss_fun = tf.losses.BinaryCrossentropy(from_logits=False)
optimizer1 = tf.keras.optimizers.Adam(1e-4)
optimizer2 = tf.keras.optimizers.Adam(1e-4)
optimizer3 = tf.keras.optimizers.Adam(1e-4)

"""
定义记录损失的变化
"""
res_trains_loss = tf.keras.metrics.Mean("res_trains_loss")
fool_trains_loss = tf.keras.metrics.Mean("fool_trains_loss")

ae_trains_loss = tf.keras.metrics.Mean("ae_trains_loss")
dis_trains_loss = tf.keras.metrics.Mean("dis_trains_loss")

def train_epoch(image_batch,label_batch):
    with tf.GradientTape() as encoder_tap,tf.GradientTape() as decoder_tap,tf.GradientTape() as dis_tap:
        
        """
        噪音特征图像素值符合正态分布
        """
        noise_batch = tf.random.normal(shape = image_batch.shape)
        encoder_out = Encoder([image_batch,noise_batch])
        imgs_fake,imgs_per = Decoder(encoder_out)
        imgs_fake2 = tf.sigmoid(imgs_fake)
        perturbation = tf.clip_by_value(imgs_per,-0.15,0.15)
        last_img = tf.add(perturbation,imgs_fake2)
        
        """
        重构损失
        """
        restructure_loss = tf.nn.sigmoid_cross_entropy_with_logits(labels = image_batch,logits = imgs_fake)
        restructure_loss = tf.reduce_sum(restructure_loss)/image_batch.shape[0]
        
        predict_one = target_net(tf.sigmoid(imgs_fake),training=False)
        fool_loss_one = fool_loss_fun(predict_one,label_batch)
        
        temp_imgs = tf.clip_by_value(last_img,0.0,1.0)
        predict_two = target_net(temp_imgs,training=False)
        fool_loss_two = fool_loss_fun(predict_two,label_batch)
        
        fool_loss = fool_loss_one*5 + fool_loss_two*10
        
        """
        判别损失
        """
        real_out = Discriminator(image_batch,training=True)
        fake_out = Discriminator(temp_imgs,training=True)
        ae_dis_loss = tf.keras.losses.BinaryCrossentropy(from_logits= True)(tf.ones_like(fake_out),fake_out)
        
        gan_dis_real_loss = tf.keras.losses.BinaryCrossentropy(from_logits= True)(tf.ones_like(real_out),real_out)
        gan_dis_fake_loss = tf.keras.losses.BinaryCrossentropy(from_logits= True)(tf.zeros_like(fake_out),fake_out)
        
        #target_net_20
        all_ae_loss = restructure_loss*10-fool_loss*5+ae_dis_loss*10
        all_dis_loss = gan_dis_real_loss+gan_dis_fake_loss
        
        #target_net_32
#         all_ae_loss = restructure_loss*0.5+fool_loss+ae_dis_loss*10
#         all_dis_loss = gan_dis_real_loss+gan_dis_fake_loss

        #target_net_w28
#         all_ae_loss = restructure_loss*0.1+fool_loss+ae_dis_loss
#         all_dis_loss = gan_dis_real_loss+gan_dis_fake_loss
        
        """
        记录损失值
        """
        res_trains_loss(restructure_loss)
        fool_trains_loss(fool_loss)
        ae_trains_loss(all_ae_loss)
        dis_trains_loss(all_dis_loss)
        
    """
    更新模型参数
    """
    encoder_gard = encoder_tap.gradient(all_ae_loss,Encoder.trainable_variables)
    decoder_grad = decoder_tap.gradient(all_ae_loss,Decoder.trainable_variables)
    dis_grad = dis_tap.gradient(all_dis_loss,Discriminator.trainable_variables)
    
    optimizer1.apply_gradients(zip(encoder_gard,Encoder.trainable_variables))    
    optimizer2.apply_gradients(zip(decoder_grad,Decoder.trainable_variables))
    optimizer3.apply_gradients(zip(dis_grad,Discriminator.trainable_variables))

def generator():
    input_img = X_train[np.random.randint(X_train.shape[0])]
    input_img = np.expand_dims(input_img,0)
    input_noise = np.random.normal(size = input_img.shape)
    en_output = Encoder.predict([input_img,input_noise])
    imgs_fake,imgs_per = Decoder.predict(en_output)
    perturbation = tf.clip_by_value(imgs_per,-0.15,0.15)
    imgs_fake2 = tf.sigmoid(imgs_fake)
    last_img = tf.add(perturbation,imgs_fake2)
    last_img = tf.clip_by_value(last_img,0.0,1.0)
    p_label0 = np.argmax(target_net.predict(input_img))
    p_label = np.argmax(target_net.predict(last_img))
    plt.figure(figsize = (2,3))
    ax1 = plt.subplot(1,2,1)
    ax1.axis("off")
    ax1.imshow(input_img[0])
    ax2 = plt.subplot(1,2,2)
    ax2.axis("off")
    ax2.imshow(last_img[0])
    print("\n原始样本lable:{}-----对抗样本lable:{}".format(labels_name[p_label0],labels_name[p_label]))
    plt.show()

"""
迭代的次数
"""
EPOCHS = 65

def train():
    for epoch in tqdm(range(EPOCHS)):
        for image_batch,label_batch in train_dataset:
            print('*',end = "")
            train_epoch(image_batch,label_batch)
        generator()
        print()
        print("restructure_loss",res_trains_loss.result().numpy())
        print("fool_loss",fool_trains_loss.result().numpy())
        print("ae_gen_loss",ae_trains_loss.result().numpy())
        print("ad_dis_loss",dis_trains_loss.result().numpy())
        res_trains_loss.reset_states()
        fool_trains_loss.reset_states()
        ae_trains_loss.reset_states()
        dis_trains_loss.reset_states()
        if epoch % 5 == 0:
            print("ResNet20网络攻击成功率:",cul_attck_net(nums=10000,target_net = target_net_20))
            print("ResNet32网络攻击成功率:",cul_attck_net(nums=10000,target_net = target_net_32))
            print("WResNet28网络攻击成功率:",cul_attck_net(nums=10000,target_net = target_net_w28))


if __name__ == '__main__':
	train()





